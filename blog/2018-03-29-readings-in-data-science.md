---
title: 50 years of Data Science in Review
author: Brett Johnson
date: 2018-03-29
slug: 50-years-of-data-science
categories: []
tags: []
subtitle: ''
---

## 50 years of Data Science by D. Donoho

John Tukey is a sort of statistical forefather that every statistics student learns about. He's responsible for many common statistical tools (Tukey HSD, jack-knifing, and the box-plot), and he had a significant impact on the development of the field of statistics. But his contributions weren't limited to theory. In 1962 he wrote a paper called 'The Future of Data Analysis', where he actually coins the term data analysis. This was big in those days because he suggested the field of statistics needs to broaden it's scope from purely theoretical musings on inferential statistics, to the entire practice of analyzing data. 

The term Data Science can trace its roots back to John Tukey, and while people argue about whether Data Science is a real field or if its just what statisticians have been doing well then it becomes clear that Data Science encompasses the traditional statistics discipline but it encompasses much more, including—most interestingly to me—the idea that the workflow to facilitate a data analysis is as much a part of the data analysis as the statistical framework an analyst might apply. 

I've always been fascinated by process. I'm a firm believer that the end product you create is merely the result of the process that you undertook. If your process lacks sufficient rigour, checks and balances, intellect, documentation, and reproduce-ability then the final product must be placed in question. Tukey even goes so far as describing the act of conducting science on the way we conduct a data analysis, being very much part of data analysis. I really like this idea. I am so interested in the process by which we collect our data, the treatment of that data, the storage of that data, how we share that data, how we analyze that data, and most importantly how we make the most rigorous application of every step of this data analysis as easy and intuitive as possible for the analyst and the consumer of the analysis.

Donoho describes a 'Greater Data Science' discipline as a broader subject area that merges and re-labels some of the divisions that have been proposed before. Donoho's six divisions are: 1. Data Gathering, Preparation, and Exploration 2. Data Representation and Transformation 3. Computing with Data 4. Data Modelling 5. Data Visualization and Presentation 6. Science about Data Science

I'd like to look at the first and last divisions in some detail because they are the most interesting to me and I think expand the field of Data Science the most and leave no doubt that Data Science is a comprehensive scientific discipline.

### Data Gathering, Preparation, and Exploration:

This sounds pretty straightforward, and it can be — with very careful forethought. Careful planing and close attention during data gathering ensures: preparation runs smoothly, and few inconsistencies appear during exploration of the data. However, collecting data is usually an iterative process. You realize how you need to treat the data, the more you think about it. Depending on when you manage to find the time to think very closely about how you are collecting data, preparation and exploration gets more and more time consuming the longer you wait and the more data you collect. The reality is that data gathering in and of itself is a well developed century old process of experimental design which is really must be meticulously thought out to avoid bias, and is really the bread and butter of every scientist. 

With the widespread and growing availability of data on the internet, data gathering has grown a new branch; scraping websites, twitter, Facebook, click logs etc... But these huge data sources are fraught with caveats. If careful attention is not paid to how the data are being collected then the vast quantity of data will not overcome the lack of quality. Regardless of what type of data you collect, if you manage to collect data in as un-biased a method as possible, then preparing the structure of the data requires attention. 

Hopefully some forethought was given to how the data ought to be structured before you started collecting it, otherwise preparing the data into a manageable format can be a tremendous amount of work. Either way once you have your data prepared you will want to explore it to make sure it behaves in the way you expect, ie. there are no issues joining tables together, you aren't missing data, you don't have NAs where you ought to have data and so on. This process of gathering, preparing, and exploring data can take months to be sure that the data are of the highest quality, and too often this step goes without enough attention in hopes that the data are all good so that the researcher can skip ahead to data modelling and data visualization and presentation.

### Science about Data Science:

I think this is also very interesting. Using the scientific method to discover truths about how to optimize the process of conducting science could be the most important concept in the field. Without introspection we may find that we devolve into whatever is new, easy, or trending. Close analysis to the tools we use, the and the data preparation steps we undertake is equally important to model selection.

Ultimately I think my interest in Data Science stems from my obsession with process, and how science is conducted. I've always be task oriented, and process driven. And it seems to me that's what Data Science, and perhaps all science is actually about.



